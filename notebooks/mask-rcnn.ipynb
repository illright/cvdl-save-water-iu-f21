{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "Copy of CVDL: Mask R-CNN.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjJGGPqfIdbz"
      },
      "source": [
        "# YOLO, so Save Water â€“ Mask R-CNN edition\n",
        "\n",
        "This notebook will assess the performance of Mask R-CNN for detection of taps in pictures, differentiating between taps with and without running water.\n",
        "\n",
        "This notebook is heavily based on the example notebook [`train_shapes.ipynb` from the Mask R-CNN GitHub repository](https://github.com/matterport/Mask_RCNN/blob/master/samples/shapes/train_shapes.ipynb). \n",
        "\n",
        "Before running the notebook, upload the dataset file `YOLO, so Save Water.zip` into `/content/` (the main directory of the file browser)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sMpMCScUZyr"
      },
      "source": [
        "## Step 0. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTGQ2fq9XX34"
      },
      "source": [
        "First, let's clone the Mask R-CNN repository to get ahold of its implementation and install its dependencies. Mask R-CNN requires quite old versions of dependencies, so we must explicitly downgrade :("
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMXXUY96USbx"
      },
      "source": [
        "![ ! -d /content/Mask_RCNN ] && git clone https://github.com/matterport/Mask_RCNN.git\n",
        "%cd /content/Mask_RCNN\n",
        "%pip install -r requirements.txt\n",
        "%pip install 'tensorflow < 2.0.0' 'keras == 2.1.5' 'scikit-image == 0.16.2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgMxYGz-XgeR"
      },
      "source": [
        "We will also download the pretrained weights to speed up learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQxaOu6m5ZS9"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "mask_rcnn_dir = '/content/Mask_RCNN'\n",
        "sys.path.append(mask_rcnn_dir)\n",
        "\n",
        "from mrcnn.config import Config\n",
        "from mrcnn import utils\n",
        "import mrcnn.model as modellib\n",
        "from mrcnn import visualize\n",
        "from mrcnn.model import log\n",
        "\n",
        "%matplotlib inline \n",
        "\n",
        "# Directory to save logs and trained model\n",
        "MODEL_DIR = os.path.join(mask_rcnn_dir, \"logs\")\n",
        "\n",
        "# Local path to trained weights file\n",
        "COCO_MODEL_PATH = os.path.join(mask_rcnn_dir, \"mask_rcnn_coco.h5\")\n",
        "# Download COCO trained weights from Releases if needed\n",
        "if not os.path.exists(COCO_MODEL_PATH):\n",
        "    utils.download_trained_weights(COCO_MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYnH-MJ0ltpj"
      },
      "source": [
        "And define a helper function for later:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVsiD5Mglxlc"
      },
      "source": [
        "def get_ax(rows=1, cols=1, size=8):\n",
        "    \"\"\"Return a Matplotlib Axes array to be used in\n",
        "    all visualizations in the notebook. Provide a\n",
        "    central point to control graph sizes.\n",
        "    \n",
        "    Change the default size attribute to control the size\n",
        "    of rendered images\n",
        "    \"\"\"\n",
        "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
        "    return ax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G428k6z55ZTA"
      },
      "source": [
        "## Step 1. Configuration & Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSK_6hscl1c6"
      },
      "source": [
        "We will now configure Mask R-CNN to our dataset and hardware. The changes from the tutorial include reducing the amount of images per GPU, changing the number of classes, image dimensions, and reducing some other characteristics to ensure we don't exceed Colab's limits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJTKVfbe5ZTA"
      },
      "source": [
        "class TapsConfig(Config):\n",
        "    # Give the configuration a recognizable name\n",
        "    NAME = \"taps\"\n",
        "\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "    # Number of classes (including background)\n",
        "    NUM_CLASSES = 1 + 2  # background + 2 kinds of taps\n",
        "\n",
        "    # Use small images for faster training. Set the limits of the small side\n",
        "    # the large side, and that determines the image shape.\n",
        "    IMAGE_MIN_DIM = 448\n",
        "    IMAGE_MAX_DIM = 448\n",
        "\n",
        "    TRAIN_ROIS_PER_IMAGE = 8\n",
        "    \n",
        "config = TapsConfig()\n",
        "config.display()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeXeEyRemHob"
      },
      "source": [
        "Now it's time to unpack and load the dataset. A custom implementation of `utils.Dataset` is provided below to account for the pecularities of storing images and their corresponding masks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7zyCDASkiM2"
      },
      "source": [
        "!unzip -qo '/content/YOLO, so Save Water.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "wNeHqnTr5ZTC"
      },
      "source": [
        "import json\n",
        "from itertools import islice\n",
        "from shutil import copy\n",
        "from os import makedirs, scandir, path\n",
        "from random import shuffle\n",
        "\n",
        "import skimage\n",
        "\n",
        "\n",
        "def split_dataset(dataset_dir, ratios=(0.8, 0.15, 0.5)):\n",
        "    images = list(scandir(path.join(dataset_dir, \"img\")))\n",
        "    shuffle(images)\n",
        "    image_iterator = iter(images)\n",
        "    directories = (\"train\", \"val\", \"test\")\n",
        "\n",
        "    for directory, ratio in zip(directories, ratios):\n",
        "        target = path.join(dataset_dir, \"./split\", directory)\n",
        "        makedirs(target)\n",
        "        makedirs(path.join(target, 'ann'))\n",
        "        for image in islice(image_iterator, int(len(images) * ratio)):\n",
        "            copy(path.join(dataset_dir, \"img\", image.name), target)\n",
        "            copy(path.join(dataset_dir, \"ann\", f\"{image.name}.json\"), path.join(target, \"ann\"))\n",
        "\n",
        "\n",
        "class TapDataset(utils.Dataset):\n",
        "    name = \"taps\"\n",
        "\n",
        "    @staticmethod\n",
        "    def split_coords(points):\n",
        "        xs: list[int] = []\n",
        "        ys: list[int] = []\n",
        "\n",
        "        for point in points:\n",
        "            xs.append(point[0])\n",
        "            ys.append(point[1])\n",
        "\n",
        "        return xs, ys\n",
        "\n",
        "    def get_class_id(self, class_name):\n",
        "        for class_info in self.class_info:\n",
        "            if class_info['name'] == class_name:\n",
        "                return class_info['id']\n",
        "\n",
        "    def load(self, dataset_dir, subset):\n",
        "        meta = json.load(open(path.join(dataset_dir, \"meta.json\")))\n",
        "        for index, img_class in enumerate(meta[\"classes\"], start=1):\n",
        "            self.add_class(TapDataset.name, index, img_class[\"title\"])\n",
        "\n",
        "        subset_dir = path.join(dataset_dir, \"split\", subset)\n",
        "        for image in scandir(subset_dir):\n",
        "            if image.is_dir():\n",
        "                continue\n",
        "\n",
        "            annotation = json.load(open(path.join(subset_dir, \"ann\", f\"{image.name}.json\")))\n",
        "            self.add_image(\n",
        "                TapDataset.name,\n",
        "                image_id=image.name,\n",
        "                path=image.path,\n",
        "                width=annotation[\"size\"][\"width\"],\n",
        "                height=annotation[\"size\"][\"height\"],\n",
        "                points=annotation['objects'][0]['points'],\n",
        "                class_name=annotation['objects'][0]['classTitle'],\n",
        "            )\n",
        "\n",
        "    def load_mask(self, image_idx):\n",
        "        image_info = self.image_info[image_idx]\n",
        "        class_id = self.get_class_id(image_info['class_name'])\n",
        "\n",
        "        if image_info[\"source\"] != TapDataset.name:\n",
        "            return super().load_mask(image_idx)\n",
        "\n",
        "        mask = np.zeros([image_info[\"height\"], image_info[\"width\"], 1], dtype=np.uint8)\n",
        "        ext_x_coords, ext_y_coords = TapDataset.split_coords(image_info['points']['exterior'])\n",
        "        rr, cc = skimage.draw.polygon(ext_x_coords, ext_y_coords)\n",
        "        mask[cc, rr, 0] = 1\n",
        "        for polygon in image_info['points']['interior']:\n",
        "            int_x_coords, int_y_coords = TapDataset.split_coords(polygon)\n",
        "            rr, cc = skimage.draw.polygon(int_x_coords, int_y_coords)\n",
        "            mask[cc, rr, 0] = 1\n",
        "\n",
        "        return mask.astype(np.bool), np.array([class_id], dtype=np.int32)\n",
        "\n",
        "    def image_reference(self, image_idx):\n",
        "        info = self.image_info[image_idx]\n",
        "        if info[\"source\"] == \"balloon\":\n",
        "            return info[\"save_water\"]\n",
        "        else:\n",
        "            super().image_reference(image_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "bsH35NQ55ZTD"
      },
      "source": [
        "!rm -rf '/content/Mask_RCNN/YOLO, so Save Water/split'\n",
        "split_dataset('/content/Mask_RCNN/YOLO, so Save Water')\n",
        "\n",
        "# Training dataset\n",
        "dataset_train = TapDataset()\n",
        "dataset_train.load('/content/Mask_RCNN/YOLO, so Save Water', 'train')\n",
        "dataset_train.prepare()\n",
        "\n",
        "# Validation dataset\n",
        "dataset_val = TapDataset()\n",
        "dataset_val.load('/content/Mask_RCNN/YOLO, so Save Water', 'val')\n",
        "dataset_val.prepare()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bGuUisH5ZTE"
      },
      "source": [
        "# Load and display random samples\n",
        "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
        "for image_id in image_ids:\n",
        "    image = dataset_train.load_image(image_id)\n",
        "    mask, class_ids = dataset_train.load_mask(image_id)\n",
        "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjvpE5NZ5ZTE"
      },
      "source": [
        "## Step 2. Model Preloading & Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "BTg2sG645ZTF"
      },
      "source": [
        "# Create model in training mode\n",
        "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
        "                          model_dir=MODEL_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "scrolled": false,
        "id": "nrqm1rZZ5ZTF"
      },
      "source": [
        "# Which weights to start with?\n",
        "init_with = \"last\"  # imagenet, coco, or last\n",
        "\n",
        "if init_with == \"imagenet\":\n",
        "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
        "elif init_with == \"coco\":\n",
        "    # Load weights trained on MS COCO, but skip layers that\n",
        "    # are different due to the different number of classes\n",
        "    # See README for instructions to download the COCO weights\n",
        "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
        "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
        "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
        "elif init_with == \"last\":\n",
        "    # Load the last model you trained and continue training\n",
        "    model.load_weights(model.find_last(), by_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xqKTneq5ZTF"
      },
      "source": [
        "## Training\n",
        "\n",
        "Train in two stages:\n",
        "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
        "\n",
        "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "QzUf4zvw5ZTG"
      },
      "source": [
        "# Train the head branches\n",
        "# Passing layers=\"heads\" freezes all layers except the head\n",
        "# layers. You can also pass a regular expression to select\n",
        "# which layers to train by name pattern.\n",
        "model.train(dataset_train, dataset_val, \n",
        "            learning_rate=config.LEARNING_RATE, \n",
        "            epochs=1, \n",
        "            layers='heads')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "_mEWeBh_5ZTG"
      },
      "source": [
        "# Fine tune all layers\n",
        "# Passing layers=\"all\" trains all layers. You can also \n",
        "# pass a regular expression to select which layers to\n",
        "# train by name pattern.\n",
        "model.train(dataset_train, dataset_val, \n",
        "            learning_rate=config.LEARNING_RATE / 10,\n",
        "            epochs=2, \n",
        "            layers=\"all\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Awk6PSec5ZTH"
      },
      "source": [
        "## Step 3. Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will recreate the model in inference mode and preload it with the last saved weights."
      ],
      "metadata": {
        "id": "Yu42igHgebo8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "O_5pl57n5ZTH"
      },
      "source": [
        "inference_config = TapsConfig()\n",
        "\n",
        "# Recreate the model in inference mode\n",
        "model = modellib.MaskRCNN(mode=\"inference\", \n",
        "                          config=inference_config,\n",
        "                          model_dir=MODEL_DIR)\n",
        "\n",
        "# Get path to saved weights\n",
        "# Either set a specific path or find last trained weights\n",
        "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
        "model_path = model.find_last()\n",
        "\n",
        "# Load trained weights\n",
        "print(\"Loading weights from \", model_path)\n",
        "model.load_weights(model_path, by_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a model example â€“ manually annotated. This is what we want the model to produce."
      ],
      "metadata": {
        "id": "2y-TlMKSejHq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nsjijxkw5ZTH"
      },
      "source": [
        "# Test on a random image\n",
        "image_id = random.choice(dataset_val.image_ids)\n",
        "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "    modellib.load_image_gt(dataset_val, inference_config, \n",
        "                           image_id, use_mini_mask=False)\n",
        "\n",
        "log(\"original_image\", original_image)\n",
        "log(\"image_meta\", image_meta)\n",
        "log(\"gt_class_id\", gt_class_id)\n",
        "log(\"gt_bbox\", gt_bbox)\n",
        "log(\"gt_mask\", gt_mask)\n",
        "\n",
        "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
        "                            dataset_train.class_names, figsize=(8, 8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here is what the model actually produces for that image:"
      ],
      "metadata": {
        "id": "QOjmC2IKeqMe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jaL-Gvi5ZTH"
      },
      "source": [
        "results = model.detect([original_image], verbose=1)\n",
        "\n",
        "r = results[0]\n",
        "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
        "                            dataset_val.class_names, r['scores'], ax=get_ax())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It leaves a lot to be desired, but nonetheless seems to perform reasonably well at least in the generation of masks. It seems like the model does better at images of taps with running water."
      ],
      "metadata": {
        "id": "BfUTM1b4evGM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clRIbyze5ZTI"
      },
      "source": [
        "# Compute VOC-Style mAP @ IoU=0.5\n",
        "# Running on 10 images. Increase for better accuracy.\n",
        "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
        "APs = []\n",
        "for image_id in image_ids:\n",
        "    # Load image and ground truth data\n",
        "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "        modellib.load_image_gt(dataset_val, inference_config,\n",
        "                               image_id, use_mini_mask=False)\n",
        "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
        "    # Run object detection\n",
        "    results = model.detect([image], verbose=0)\n",
        "    r = results[0]\n",
        "    # Compute AP\n",
        "    AP, precisions, recalls, overlaps =\\\n",
        "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
        "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
        "    APs.append(AP)\n",
        "    \n",
        "print(\"mAP: \", np.mean(APs))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}